{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"hw_backprop.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"mo4MwTZtocWC","colab_type":"text"},"source":["# Backprop своими руками"]},{"cell_type":"markdown","metadata":{"id":"ZTn-EUeLocWG","colab_type":"text"},"source":["Материалы:\n","\n","* [Andrew Karpahy: yes, you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)\n","* [Stanford CS231n](http://cs231n.stanford.edu/)\n","* [Deep Learning](http://sereja.me/f/deep_learning_goodfellow.pdf) — с 204 страницы и до прозрения\n","* [Xavier, Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)"]},{"cell_type":"markdown","metadata":{"id":"vr9onIFLocWI","colab_type":"text"},"source":["<img width='800px' src='https://cdn-images-1.medium.com/max/1600/1*q1M7LGiDTirwU-4LcFq7_Q.png'>"]},{"cell_type":"markdown","metadata":{"id":"0pNAiOscocWJ","colab_type":"text"},"source":["Иногда сети пишут на чистом C++, причём код для обучения и инференса (реального прогона в продакшне) — отдельно. Так делают, когда нужен очень быстрый отклик и высокая производительность, но это очень трудоемко.\n","\n","Большинство людей не усложняют себе жизнь и просто используют фреймворки — с ними можно просто почти декларативно описать, какие операции хотите сделать с данными, а он потом сам построит сеть и подгонит её под данные.\n","\n","Ваше задание — реализовать свой небольшой фреймворк глубокого обучения на чистом `numpy`. Основное время у вас должно уйти на вывод формул для градиентов, анализ поведения самых часто используемых слоев в современных нейросетях и прочий матан. Хотя бы один раз в жизни это надо сделать, а уже потом пользоваться готовыми абстракциями."]},{"cell_type":"markdown","metadata":{"id":"boLW-DeTocWL","colab_type":"text"},"source":["Предполагаемый порядок выполнения:\n","* Поймите на высоком уровне, как работает алгоритм backpropagation\n","* Изучите пример с логиситической регрессией, чтобы понять, что от вас в итоге хотят\n","* Изучите код `Module`\n","* `Sequential`\n","* `Linear`\n","* `SoftMax`\n","* `CrossEntropy`\n","* Протестируйте их на логситической регрессии\n","* Напишите код для решения MNIST\n","* Дописывайте остальные слои, пока не получите на нём 97%"]},{"cell_type":"markdown","metadata":{"id":"sPciq6WzocWN","colab_type":"text"},"source":["Оценивание (суммарно до 20 баллов):\n","* 5 баллов -- что-то хоть как-то обучается, MNIST на валидации >90%\n","* 2 балла -- MNIST на 95%, дальше по одному баллу за 96%, 97% и 98%. Это будет сделать намного сложнее, чем через PyTorch, потому что вам всё нужно писать самим: более сложные оптимизаторы, learning rate decay, думать про численную стабильность и т. д.\n","* По 2 балла за слои: LeakyReLU, Dropout, BatchNorm, CrossEntropy, SoftMax"]},{"cell_type":"markdown","metadata":{"id":"VrI9amBAocWO","colab_type":"text"},"source":["Советы:\n","* Чтобы лучше понять, что должно в итоге получиться, изучите «игрушечный пример» и вообще эту тетрадку, а потом начните читать `hw_framework.py`, где будет более техничное описание.\n","* Для дебага проверяйте градиенты численно — сдвигайте параметры на какой-нибудь эпсилон и смотрите разницу. Ещё можете проверить, что на одинаковых данных они дают то же, что их эквиваленты из PyTorch (можно после каждого нового слоя добавить юнит тест через `assert`).\n","* Пишите код без циклов — в питоне они очень долгие; все вычисления можно делать внутри numpy.\n","* Ограничение на срок сдачи большое — до конца всего курса, но рекомендуется закончить примерно за месяц. Дописывайте его постепенно, разбираясь, как работает каждая функция активации.\n","* Обсуждайте математику и общую архитектуру фреймворка, но не шарьте друг другу код — так не интересно."]},{"cell_type":"code","metadata":{"id":"osDsI56TocWQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rArLbzznocWU","colab_type":"text"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"9SpmBG6XocWV","colab_type":"text"},"source":["Откройте в Jupyter две тетрадки — эту и `hw_framework.ipynb`. В этой содержится train loop, а там непосредственно ваш «фреймворк», который вам ещё предстоит написать.\n","\n","Архитектура фреймворка вдохновлена PyTorch. Как всегда, если придумаете какой-то более клёвый дизайн — можете использовать его."]},{"cell_type":"code","metadata":{"id":"AR35O38uocWX","colab_type":"code","colab":{}},"source":["%run hw_framework.ipynb"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z03GIF03ocWc","colab_type":"text"},"source":["Мы будем использовать самый простой вариант градиентного спуска: просто пройдемся по всем параметрам и сделаем шаги в сторону уменьшения посчитанного заранее градиента.\n","\n","Есть [более продвинутые методы](http://ruder.io/optimizing-gradient-descent/), но пока что мы их использовать не будем."]},{"cell_type":"code","metadata":{"id":"gIkgsfAXocWd","colab_type":"code","colab":{}},"source":["def SGD(params, gradients, lr=1e-3):    \n","    for weights, gradient in zip(params, gradients):\n","        #print(type(lr), type(gradient))\n","        #print(lr, gradient)\n","        weights -= lr * gradient"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PfBYatsKocWh","colab_type":"text"},"source":["Cоздадим обертку вокруг нашего датасета (просто numpy-евские массивы), которую будем потом использовать, чтобы итерироваться по нему."]},{"cell_type":"code","metadata":{"id":"jDmJKg8hocWi","colab_type":"code","colab":{}},"source":["def loader(X, Y, batch_size):    \n","    n = X.shape[0]\n","\n","    # в начале каждой эпохи будем всё перемешивать\n","    # важно, что мы пермешиваем индексы, а не X\n","    indices = np.arange(n)\n","    np.random.shuffle(indices)\n","    \n","    for start in range(0, n, batch_size):\n","        # в конце нам, возможно, нужно взять неполный батч\n","        end = min(start + batch_size, n)\n","        \n","        batch_idx = indices[start:end]\n","    \n","        yield X[batch_idx], Y[batch_idx]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMcWZrUPocWm","colab_type":"text"},"source":["В следующих двух секциях — игрушечные примеры регрессии и классификации на синтезированных данных. В них ничего менять не надо — они нужны, чтобы отдебажить ваши слои в `hw_framework`."]},{"cell_type":"markdown","metadata":{"id":"MLPP2pyMocWn","colab_type":"text"},"source":["# Линейная регрессия"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"i2oHX9NQocWo","colab_type":"code","colab":{}},"source":["n = 1000\n","\n","X = np.random.randn(n, 10)\n","true_w = np.random.randn(10, 1)\n","Y = np.dot(X, true_w).reshape(n)# + np.random.randn()/5\n","\n","print('best_possible_mse:', np.mean(np.power(Y-np.dot(X, true_w).reshape(n), 2)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gu4bulrKocWs","colab_type":"text"},"source":["### Модель"]},{"cell_type":"code","metadata":{"id":"2bHZzM0XocWt","colab_type":"code","colab":{}},"source":["model = Sequential(\n","    Linear(10, 1),\n",")\n","\n","criterion = MSE()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsdkT55BocWx","colab_type":"text"},"source":["Можете тут потом тестировать остальные свои слои, когда их напишите."]},{"cell_type":"markdown","metadata":{"id":"G6cJy-nOocWy","colab_type":"text"},"source":["### Обучение"]},{"cell_type":"code","metadata":{"id":"P3Y7VImUocW0","colab_type":"code","colab":{}},"source":["epochs = 10\n","batch_size = 10\n","learning_rate = 1e-1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NHCVPRiocW4","colab_type":"code","colab":{}},"source":["history = []\n","\n","for i in range(epochs):\n","    for x, y_true in loader(X, Y, batch_size):\n","        # forward -- считаем все значения до функции потерь\n","        y_pred = model.forward(x)\n","        loss = criterion.forward(y_pred, y_true)\n","        \n","        #print(y_pred, y_true)\n","        #print('SUM OF SQUARES:', np.mean(np.power(y_pred-y_true, 2)))\n","    \n","        # backward -- считаем все градиенты в обратном порядке\n","        grad = criterion.backward(y_pred, y_true)\n","        model.backward(x, grad)\n","        \n","        # обновляем веса\n","        SGD(model.parameters(),\n","            model.grad_parameters(),\n","            learning_rate)\n","        \n","        #print(model.layers[0].W[0][0])\n","        print(loss)\n","        \n","        history.append(loss)\n","\n","    \n","plt.title(\"Training loss\")\n","plt.xlabel(\"iteration\")\n","plt.ylabel(\"loss\")\n","plt.plot(history, 'b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"33YJP2KeocW8","colab_type":"text"},"source":["# Логистическая регрессия\n","\n","Этот пример нужнен для теситрования классификации (`CrossEntropy` и `SoftMax`).\n","\n","Возьмем в качестве датасета точки из двух гауссиан на плоскости."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Vy-6IgGWocW9","colab_type":"code","colab":{}},"source":["n = 500\n","\n","X1 = np.random.randn(n, 2) + np.array([2, 2])\n","X2 = np.random.randn(n, 2) + np.array([-2, -2])\n","X = np.vstack([X1, X2])\n","\n","Y = np.concatenate([np.ones(n), np.zeros(n)])[:, None]\n","Y = np.hstack([Y, 1-Y])\n","\n","plt.scatter(X[:,0], X[:,1], c=Y[:,0])\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6ga19VxocXB","colab_type":"text"},"source":["Обратите внимание на `y`. Он в формате one-hot: у каждого вектора все нули, кроме одной единицы.\n","\n","Выходные данные в таком формате упростят написание `CrossEntropy`."]},{"cell_type":"markdown","metadata":{"id":"zNDguGQXocXD","colab_type":"text"},"source":["### Модель\n","\n","Логистическая регрессия — это тоже как бы маленькая нейронка: линейный слой, софтмакс, и максимизируем правдоподобие."]},{"cell_type":"code","metadata":{"id":"fdKlc0EgocXE","colab_type":"code","colab":{}},"source":["model = Sequential(\n","    Linear(2, 2),\n","    SoftMax()\n",")\n","\n","criterion = CrossEntropy()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMoT8yXCocXI","colab_type":"text"},"source":["### Обучение"]},{"cell_type":"code","metadata":{"id":"KIk7RXUrocXJ","colab_type":"code","colab":{}},"source":["epochs = 10\n","batch_size = 16\n","learning_rate = 1e-2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"6RU473D9ocXN","colab_type":"code","colab":{}},"source":["history = []\n","\n","for i in range(epochs):\n","    for x, y_true in loader(X, Y, batch_size):\n","        # forward: считаем все значения до функции потерь\n","        y_pred = model.forward(x)\n","        loss = criterion.forward(y_pred, y_true)\n","    \n","        # backward: считаем все градиенты в обратном порядке\n","        grad = criterion.backward(y_pred, y_true)\n","        model.backward(x, grad)\n","        \n","        # обновляем веса\n","        SGD(model.parameters(),\n","            model.grad_parameters(),\n","            learning_rate)\n","        \n","        # логгируем лосс\n","        # history.append(loss)\n","\n","    \n","plt.title(\"Training loss\")\n","plt.xlabel(\"iteration\")\n","plt.ylabel(\"loss\")\n","plt.plot(history, 'b')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mc_7Av6uocXR","colab_type":"text"},"source":["Мы тут пропустили много важных деталей: валидация, подсчет точности (кроссэнтропия не очень информативна), регуляризация. Вам всё это нужно будет реализовать потом самим."]},{"cell_type":"markdown","metadata":{"id":"tmqQxNhFocXS","colab_type":"text"},"source":["# Теперь сами"]},{"cell_type":"code","metadata":{"id":"BRYIbyvzocXT","colab_type":"code","colab":{}},"source":["import os\n","from sklearn.datasets import fetch_mldata\n","# эти библиотеки нужны только для того, чтобы скачать MNISt\n","\n","if os.path.exists('mnist.npz'):\n","    with np.load('mnist.npz', 'r') as data:\n","        X = data['X']\n","        y = data['y']\n","else:\n","    mnist = fetch_mldata(\"mnist-original\")\n","    # очень важно его отнормировать -- см. Linear в hw_framework\n","    X = mnist.data / 255.0\n","    y = mnist.target\n","    np.savez('mnist.npz', X=X, y=y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPZV7JdYocXa","colab_type":"text"},"source":["Переведите лейблы в one-hot."]},{"cell_type":"code","metadata":{"id":"A0n63WKSocXc","colab_type":"code","colab":{}},"source":["# ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d726biJBocXj","colab_type":"text"},"source":["Разделите датасет на train и validation."]},{"cell_type":"code","metadata":{"id":"VDMMhl24ocXk","colab_type":"code","colab":{}},"source":["# ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XHv3CVCIocXo","colab_type":"text"},"source":["Теперь напишите модель и train loop. Можете начать с адаптации предыдущего примера.\n","\n","Дальше начинается творческая часть и настоящий Deep Learning:\n","* поиграйтесь с архитектурами;\n","* поиграйтесь с learning rate и batch_size;\n","* сделайте learning rate decay;\n","* сделайте data augmentation.\n","\n","Have fun. Дедлайн — не две недели, а до конца курса."]},{"cell_type":"code","metadata":{"id":"q4XW6uFEocXp","colab_type":"code","colab":{}},"source":["# ..."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EHGcq3iMocXs","colab_type":"code","colab":{}},"source":["# ..."],"execution_count":0,"outputs":[]}]}