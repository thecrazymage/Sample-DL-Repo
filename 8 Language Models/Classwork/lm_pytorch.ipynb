{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with language modelling\n",
    "\n",
    "Если вы пропустили лекцию, то посмотрите слайды к ней — они где-то есть. Также полезно почитать:\n",
    "\n",
    "* [Unreasonable effectiveness of RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (Andrej Karpathy)\n",
    "* [Официальный пример от PyTorch](https://github.com/pytorch/examples/tree/master/word_language_model)\n",
    "\n",
    "Рекомендуется заранее всё прочитать, чтобы понять, что от вас хотят. При желании, можете переписать всё так, как подсказывает ваше сердце.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг (3 балла)\n",
    "\n",
    "Возьмите какие-нибудь сырые данные. Википедия, «Гарри Поттер», «Игра Престолов», тексты Монеточки, твиты Тинькова — что угодно.\n",
    "\n",
    "Для простоты будем делать char-level модель. Выкиньте из текстов все ненужные символы (можете оставить только алфавит и, пунктуацию). Сопоставьте всем различным символам свой номер. Удобно это хранить просто в питоновском словаре (`char2idx`). Для генерации вам потребуется ещё и обратный словарь (`idx2char`). Вы что-то такое должны были писать на вступительной — можете просто переиспользовать код оттуда.\n",
    "\n",
    "Заранее зарезервируйте айдишники под служебные символы: `<START>`, `<END>`, `<PAD>`, `<UNK>`.\n",
    "\n",
    "Клёво будет написать отдельный класс, который делает токенизацию и детокенизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.char2idx = # ...\n",
    "        self.idx2char = # ...\n",
    "    \n",
    "    def tokenize(self, sequence):\n",
    "        # выполните какой-то базовый препроцессинг\n",
    "        # например, оставьте только алфавит и пунктуацию\n",
    "        return [self.char2idx[char] for char in sequence]\n",
    "    \n",
    "    def detokenize(self, sequence):\n",
    "        return ''.join([self.idx2char[idx] for idx in sequence])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        # загрузите данные\n",
    "        data = # ...\n",
    "\n",
    "        # обучите вокаб\n",
    "        self.vocab = Vocab(data)\n",
    "\n",
    "        # разделите данные на отдельные сэмплы для обучения\n",
    "        # (просто список из сырых строк)\n",
    "        self.data = # ...\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __get__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample = self.vocab.tokenize(sample)\n",
    "        sample = # паддинг до maxlen (см. дальше)\n",
    "        sample = # сконвертируйте в LongTensor\n",
    "        target = # нужно предсказать эту же последовательность со сдвигом 1\n",
    "        return sample, targer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас какой-то большой массив текста (скажем, статьи Википедии), вы можете просто нарезать из него кусочки фиксированной длины и так их подавать в модель.\n",
    "\n",
    "Если же вы хотите приключений, то можно разбить этот текст на предложения (`nltk.sent_tokenize`), и тогда все примеры будут разной длины. По соображениям производительности, вы не хотите использовать самые длинные и самые короткие сэмплы, поэтому имеет смысл обрезать их по длине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(x) for x in text], bins=350)\n",
    "plt.title('Распределение числа слов в предложениях.')\n",
    "plt.xlabel('Число слов')\n",
    "plt.xlim((0, 300))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 40  # предложения с меньшим количеством символов не будут рассматриваться\n",
    "max_len = 150 # предложения с большим количеством символов будут обрезаться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём на обучение и валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset()\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель (3 балла)\n",
    "\n",
    "Примерно такое должно зайти:\n",
    "\n",
    "* Эмбеддинг\n",
    "* LSTM / GRU\n",
    "* Линейный слой\n",
    "* Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, tie_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(ntoken, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        if tie_weights:\n",
    "            # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "            # https://arxiv.org/abs/1608.05859\n",
    "            assert hidden_dim == embedding_dim\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # начальный хидден должен быть нулевой\n",
    "        # (либо хоть какой-то константный для всего обучения)\n",
    "        return torch.zeros(batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 1e-3 \n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = LM(\n",
    "    vocab_size = len(dataset.vocab),\n",
    "    input_dim = 128,\n",
    "    hidden_dim = 128,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.1,\n",
    "    tie_weights= True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train = DataLoader(train_set, batch_size=batch_size, shufle=True)\n",
    "test = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    for x, y in train:\n",
    "        model.train()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 0. Распакуйте данные на нужное устройство\n",
    "        # 1. Инициилизируйте hidden\n",
    "        hidden = model.init_hidden(len(x))\n",
    "        # 2. Прогоните данные через модель, получите предсказания на каждом токене\n",
    "        output, hidden = model(data, hidden)\n",
    "        # 3. Посчитайте лосс (maxlen независимых классификаций) и сделайте backward()\n",
    "        # 4. Клипните градиенты -- у RNN-ок с этим часто бывают проблемы\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "        # 5. Залоггируйте лосс куда-нибудь\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    for x in test:\n",
    "        model.eval()\n",
    "        \n",
    "        # сдесь нужно сделать то же самое, только без backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Спеллчекер (3 балла)\n",
    "\n",
    "Из языковой модели можно сделать простенький спеллчекер: можно визуализировать лоссы на каждом символе (либо какой-нибудь другой показатель неуверенности).\n",
    "\n",
    "Бонус: можете усреднить перплексии по словам и выделять их, а не отдельные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def print_colored(sequence, intensities, delimeter=''):\n",
    "    html = delimeter.join([\n",
    "        f'<span style=\"background: rgb({255}, {255-x}, {255-x})\">{c}</span>'\n",
    "        for c, x in zip(sequence, intensities) \n",
    "    ])\n",
    "    display(HTML(html))\n",
    "\n",
    "print_colored('Налейте мне экспрессо'.split(), [0, 0, 100], ' ')\n",
    "\n",
    "sequence = 'Эту домашку нужно сдать втечении двух недель'\n",
    "intensities = [0]*len(sequence)\n",
    "intensities[25] = 50\n",
    "intensities[26] = 60\n",
    "intensities[27] = 70\n",
    "intensities[31] = 150\n",
    "print_colored(sequence, intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck(sequence):\n",
    "    model.eval()\n",
    "    \n",
    "    # векторизуйте sequence; паддинги делать не нужно\n",
    "    sequence = ...\n",
    "    \n",
    "    # прогоните модель и посчитайте лосс, но не усредняйте\n",
    "    # с losses можно что-нибудь сделать для визуализации; например, в какую-нибудь степень возвести\n",
    "    losses = # ...\n",
    "    \n",
    "    print_colored(sequence, np.array(spellcheck_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['В этом претложениии очен много очепяток.', \n",
    "             'Здесь появилась лишнняя буква.', \n",
    "             'В этом предложении все нормально.', \n",
    "             'Чтонибудь пишеться чериз дефис.', \n",
    "             'Слова нрпдзх не сущесдвует.']\n",
    "\n",
    "for sequence in sequences:\n",
    "    spellcheck(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация предложений (3 балла)\n",
    "\n",
    "* Поддерживайте hidden state при генерации. Не пересчитывайте ничего больше одного раза.\n",
    "* Прикрутите температуру: это когда при сэмплировании все логиты (то, что перед софтмаксом) делятся на какое-то число (по умолчанию 1, тогда ничего не меняется). Температура позволяет делать trade-off между разнообразием и правдоподобием (подробнее — см. блог Карпатого).\n",
    "* Ваша реализация должна уметь принимать строку seed — то, с чего должно начинаться сгенерированная строка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(num_tokens, seed=\"\", temperature=1.0):\n",
    "    model.eval()\n",
    "        \n",
    "    hidden = model.init_hidden()\n",
    "    input = # векторизауйте seed, чтобы на первом этап получить нужный hidden    \n",
    "    \n",
    "    continuation = ''\n",
    "    \n",
    "    for _ in range(num_tokens)\n",
    "        output, hidden = model(input, hidden)\n",
    "        \n",
    "        token_probas = output.squeeze().div(temperature).exp().cpu()\n",
    "        token = torch.multinomial(token_probas, 1)[0]\n",
    "        \n",
    "        continuation += # допишите соответствующий символ\n",
    "        input = # обновите input\n",
    "    \n",
    "    return continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginnings = ['Шел медведь по лесу', \n",
    "              'Встретились англичанин, американец и русский. Англичанин говорит:',\n",
    "              'Так вот, однажды качки решили делать ремонт',\n",
    "              'Поручик Ржевский был',\n",
    "              'Идёт Будда с учениками по дороге',\n",
    "              'Мюллер: Штирлиц, где вы были в 1938 году?',\n",
    "              'Засылают к нам американцы шпиона под видом студента',\n",
    "              'Подъезжает электричка к Долгопе:']\n",
    "\n",
    "for beginning in beginnings:\n",
    "    print(f'{beginning}... {sample(10, beginning)})\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
